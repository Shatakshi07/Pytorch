{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7)\n",
      "0\n",
      "7\n",
      "tensor([7, 7])\n",
      "1\n",
      "torch.Size([2])\n",
      "tensor([[ 7,  8],\n",
      "        [ 9, 10]])\n",
      "2\n",
      "torch.Size([2, 2])\n",
      "tensor([[[1, 2, 3],\n",
      "         [3, 6, 9],\n",
      "         [2, 4, 5]]])\n",
      "3\n",
      "torch.Size([1, 3, 3])\n",
      "tensor([[0.6060, 0.0941, 0.1691, 0.4783],\n",
      "        [0.3388, 0.2963, 0.2402, 0.5690],\n",
      "        [0.9602, 0.8189, 0.2040, 0.6620]])\n",
      "torch.float32\n",
      "torch.Size([224, 224, 3])\n",
      "3\n",
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n",
      "torch.float32\n",
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n",
      "torch.float32\n",
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#print(torch.__version__)\n",
    "\n",
    "#scalar\n",
    "scalar=torch.tensor(7)\n",
    "print(scalar)\n",
    "print(scalar.ndim)\n",
    "print(scalar.item())# Get the Python number within a tensor (only works with one-element tensors)\n",
    "\n",
    "# Vector\n",
    "vector = torch.tensor([7, 7])\n",
    "print(vector)\n",
    "print(vector.ndim )# Check the number of dimensions of vector\n",
    "print(vector.shape )# Check shape of vector\n",
    "\n",
    "# Matrix\n",
    "MATRIX = torch.tensor([[7, 8], \n",
    "                       [9, 10]])\n",
    "print(MATRIX)\n",
    "print(MATRIX.ndim) # Check number of dimensions\n",
    "print(MATRIX.shape)\n",
    "\n",
    "# Tensor\n",
    "TENSOR = torch.tensor([[[1, 2, 3],\n",
    "                        [3, 6, 9],\n",
    "                        [2, 4, 5]]])\n",
    "print(TENSOR)\n",
    "print(TENSOR.ndim) # Check number of dimensions for TENSOR\n",
    "print(TENSOR.shape) # Check shape of TENSOR\n",
    "# it outputs torch.Size([1, 3, 3]).\n",
    "#The dimensions go outer to inner.\n",
    "#That means there's 1 dimension of 3 by 3.\n",
    "\n",
    "# Create a random tensor of size (3, 4)\n",
    "random_tensor = torch.rand(size=(3, 4))\n",
    "print(random_tensor)\n",
    "print(random_tensor.dtype)\n",
    "\n",
    "# Create a random tensor of size (224, 224, 3)\n",
    "random_image_size_tensor = torch.rand(size=(224, 224, 3))\n",
    "print(random_image_size_tensor.shape)\n",
    "print(random_image_size_tensor.ndim)\n",
    "\n",
    "# Create a tensor of all zeros\n",
    "zeros = torch.zeros(size=(3, 4))\n",
    "print(zeros)\n",
    "print(zeros.dtype)\n",
    "\n",
    "# Create a tensor of all ones\n",
    "ones = torch.ones(size=(3, 4))\n",
    "print(ones)\n",
    "print(ones.dtype)\n",
    "\n",
    "#Sometimes you might want a range of numbers, such as 1 to 10 or 0 to 100.\n",
    "# Create a range of values 0 to 10\n",
    "zero_to_ten = torch.arange(start=0, end=10, step=1)\n",
    "print(zero_to_ten)\n",
    "\n",
    "# Can also create a tensor of zeros similar to another tensor\n",
    "ten_zeros = torch.zeros_like(input=zero_to_ten) # will have same shape\n",
    "print(ten_zeros)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datatype for tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3])\n",
      "torch.float32\n",
      "cpu\n",
      "torch.float16\n",
      "torch.float16\n",
      "tensor([[0.0938, 0.3629, 0.9576, 0.3273],\n",
      "        [0.6843, 0.1713, 0.8480, 0.2957],\n",
      "        [0.0247, 0.6244, 0.7712, 0.5228]])\n",
      "Shape of tensor: torch.Size([3, 4])\n",
      "Shape of tensor: torch.Size([3, 4])\n",
      "Datatype of tensor: torch.float32\n",
      "Device tensor is stored on: cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "#Note: tensor datatypes is one of the 3 big errors we can run into with pytorch and deeplearning:\n",
    " #1.Tensors not right datatype\n",
    " #2.Tensors not right shape\n",
    " #3.Tensors not on the  right device\n",
    "\n",
    " #Precision in computing wikipedia\n",
    "\n",
    "# Default datatype for tensors is float32\n",
    "float_32_tensor = torch.tensor([3.0, 6.0, 9.0],\n",
    "                               dtype=None, # defaults to None, which is torch.float32 or whatever datatype is passed\n",
    "                               device=None, #can use \"cuda\", defaults to None, which uses the default tensor type\n",
    "                               requires_grad=False) # if True, operations performed on the tensor are recorded \n",
    "print(float_32_tensor.shape)\n",
    "print(float_32_tensor.dtype)\n",
    "print(float_32_tensor.device)\n",
    "\n",
    "float_16_tensor2=float_32_tensor.type(torch.float16)\n",
    "print(float_16_tensor2.dtype)\n",
    "\n",
    "#Different datatype for tensor , CHECK FOR OTHER DATATYPES ON OFFICIAL WEBSITE TOO\n",
    "float_16_tensor = torch.tensor([3.0, 6.0, 9.0], dtype=torch.float16) # torch.half would also work\n",
    "print(float_16_tensor.dtype)\n",
    "\n",
    "#one of tensors is torch.float32 and the other is torch.float16 (PyTorch often likes tensors to be the same format).\n",
    "#Or one of your tensors is on the CPU and the other is on the GPU (PyTorch likes calculations between tensors to be on the same device).\n",
    "\n",
    "# Create a tensor\n",
    "# Find out details (or tensor attributes) about it\n",
    "some_tensor = torch.rand(3, 4)\n",
    "print(some_tensor)\n",
    "print(f\"Shape of tensor: {some_tensor.shape}\")\n",
    "print(f\"Shape of tensor: {some_tensor.size()}\")#gives same o/p as above\n",
    "print(f\"Datatype of tensor: {some_tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {some_tensor.device}\") # will default to CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MANIPULATING TENSORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Create a tensor of values and add a number to it\n",
    "tensor = torch.tensor([1, 2, 3])\n",
    "tensor + 10\n",
    "# Multiply it by 10\n",
    "tensor * 10\n",
    "# Tensors don't change unless reassigned\n",
    "tensor\n",
    "# Subtract and reassign\n",
    "tensor = tensor - 10\n",
    "tensor\n",
    "# Add and reassign\n",
    "tensor = tensor + 10\n",
    "tensor\n",
    "# Can also use torch functions\n",
    "torch.multiply(tensor, 10)\n",
    "# Original tensor is still unchanged \n",
    "tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MATRIX MULTIPLICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3]) * tensor([1, 2, 3])\n",
      "Equals: tensor([1, 4, 9])\n",
      "tensor(14)\n",
      "tensor(14)\n",
      "Execution time: 0.000518798828125 seconds\n",
      "tensor(14)\n",
      "Execution time: 0.0004420280456542969 seconds\n",
      "Original shapes: tensor_A = torch.Size([3, 2]), tensor_B = torch.Size([3, 2])\n",
      "\n",
      "New shapes: tensor_A = torch.Size([3, 2]) (same as above), tensor_B.T = torch.Size([2, 3])\n",
      "\n",
      "Multiplying: torch.Size([3, 2]) * torch.Size([2, 3]) <- inner dimensions match\n",
      "\n",
      "Output:\n",
      "\n",
      "tensor([[ 27.,  30.,  33.],\n",
      "        [ 61.,  68.,  75.],\n",
      "        [ 95., 106., 117.]])\n",
      "\n",
      "Output shape: torch.Size([3, 3])\n",
      "tensor([[ 27.,  30.,  33.],\n",
      "        [ 61.,  68.,  75.],\n",
      "        [ 95., 106., 117.]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Element-wise multiplication (each element multiplies its equivalent, index 0->0, 1->1, 2->2)\n",
    "print(tensor, \"*\", tensor)\n",
    "print(\"Equals:\", tensor * tensor)\n",
    "\n",
    "# Matrix multiplication\n",
    "print(torch.matmul(tensor, tensor))\n",
    "# Matrix multiplication by hand \n",
    "# (avoid doing operations with for loops at all cost, they are computationally expensive)\n",
    "import time\n",
    "start_time = time.time()\n",
    "value = 0\n",
    "for i in range(len(tensor)):\n",
    "  value += tensor[i] * tensor[i]\n",
    "print(value)\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time} seconds\")\n",
    "\n",
    "start_time = time.time()\n",
    "print(torch.matmul(tensor, tensor))#FASTER\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time} seconds\")\n",
    "\n",
    "# Shapes need to be in the right way  \n",
    "tensor_A = torch.tensor([[1, 2],\n",
    "                         [3, 4],\n",
    "                         [5, 6]], dtype=torch.float32)\n",
    "\n",
    "tensor_B = torch.tensor([[7, 10],\n",
    "                         [8, 11], \n",
    "                         [9, 12]], dtype=torch.float32)\n",
    "# The operation works when tensor_B is transposed\n",
    "print(f\"Original shapes: tensor_A = {tensor_A.shape}, tensor_B = {tensor_B.shape}\\n\")\n",
    "print(f\"New shapes: tensor_A = {tensor_A.shape} (same as above), tensor_B.T = {tensor_B.T.shape}\\n\")\n",
    "print(f\"Multiplying: {tensor_A.shape} * {tensor_B.T.shape} <- inner dimensions match\\n\")\n",
    "print(\"Output:\\n\")\n",
    "output = torch.matmul(tensor_A, tensor_B.T)\n",
    "print(output) \n",
    "print(f\"\\nOutput shape: {output.shape}\")\n",
    "\n",
    "# torch.mm is a shortcut for matmul\n",
    "print(torch.mm(tensor_A, tensor_B.T))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TENSOR AGGREGATION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0, 10, 20, 30, 40, 50, 60, 70, 80, 90])\n",
      "torch.int64\n",
      "Minimum: 0\n",
      "Maximum: 90\n",
      "Mean: 45.0\n",
      "Sum: 450\n",
      "tensor(90)\n",
      "tensor(0)\n",
      "tensor(45.)\n",
      "tensor(450)\n",
      "Tensor: tensor([10, 20, 30, 40, 50, 60, 70, 80, 90])\n",
      "Index where max value occurs: 8\n",
      "Index where min value occurs: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([10, 20, 30, 40, 50, 60, 70, 80, 90], dtype=torch.int8)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Create a tensor\n",
    "x = torch.arange(0, 100, 10)\n",
    "print(x)\n",
    "print(x.dtype)\n",
    "print(f\"Minimum: {x.min()}\")\n",
    "print(f\"Maximum: {x.max()}\")\n",
    "# print(f\"Mean: {x.mean()}\") # this will error\n",
    "print(f\"Mean: {x.type(torch.float32).mean()}\") # won't work without float32 datatype\n",
    "print(f\"Sum: {x.sum()}\")\n",
    "print(torch.max(x))\n",
    "print(torch.min(x))\n",
    "print(torch.mean(x.type(torch.float32)))\n",
    "print(torch.sum(x))\n",
    "\n",
    "#Positional min/max\n",
    "# Create a tensor\n",
    "tensor = torch.arange(10, 100, 10)\n",
    "print(f\"Tensor: {tensor}\")\n",
    "\n",
    "# Returns index of max and min values\n",
    "print(f\"Index where max value occurs: {tensor.argmax()}\")\n",
    "print(f\"Index where min value occurs: {tensor.argmin()}\")\n",
    "\n",
    "#Change tensor datatype\n",
    "# Create a tensor and check its datatype\n",
    "tensor = torch.arange(10., 100., 10.)\n",
    "tensor.dtype\n",
    "# Create a float16 tensor\n",
    "tensor_float16 = tensor.type(torch.float16)\n",
    "tensor_float16\n",
    "# Create a int8 tensor\n",
    "tensor_int8 = tensor.type(torch.int8)\n",
    "tensor_int8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reshaping, stacking, squeezing and unsqueezing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous tensor: tensor([[5., 2., 3., 4., 5., 6., 7.]])\n",
      "Previous shape: torch.Size([1, 7])\n",
      "\n",
      "New tensor: tensor([5., 2., 3., 4., 5., 6., 7.])\n",
      "New shape: torch.Size([7])\n",
      "Previous tensor: tensor([5., 2., 3., 4., 5., 6., 7.])\n",
      "Previous shape: torch.Size([7])\n",
      "\n",
      "New tensor: tensor([[5., 2., 3., 4., 5., 6., 7.]])\n",
      "New shape: torch.Size([1, 7])\n",
      "Previous shape: torch.Size([224, 224, 3])\n",
      "New shape: torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#torch.reshape(input, shape)\tReshapes input to shape (if compatible), can also use torch.Tensor.reshape().\n",
    "#Tensor.view(shape)\t            Returns a view of the original tensor in a different shape but shares the same data and memory as the original tensor.\n",
    "#torch.stack(tensors, dim=0)\tConcatenates a sequence of tensors along a new dimension (dim), all tensors must be same size.\n",
    "#torch.squeeze(input)\t        Squeezes input to remove all the dimenions with value 1.\n",
    "#torch.unsqueeze(input, dim)\tReturns input with a dimension value of 1 added at dim.\n",
    "#torch.permute(input, dims)\t    Returns a view of the original input with its dimensions permuted (rearranged) to dims.\n",
    "\n",
    "x = torch.arange(1., 8.)\n",
    "x, x.shape\n",
    "# Add an extra dimension\n",
    "x_reshaped = x.reshape(1, 7)\n",
    "x_reshaped, x_reshaped.shape\n",
    "# Change view (keeps same data as original but changes view)\n",
    "# See more: https://stackoverflow.com/a/54507446/7900723\n",
    "z = x.view(1, 7)\n",
    "z, z.shape\n",
    "# Changing z changes x\n",
    "z[:, 0] = 5\n",
    "z, x\n",
    "\n",
    "# Stack tensors on top of each other\n",
    "x_stacked = torch.stack([x, x, x, x], dim=0) # try changing dim to dim=1 and see what happens\n",
    "x_stacked\n",
    "x_stacked = torch.stack([x, x, x, x], dim=1) # try changing dim to dim=1 and see what happens\n",
    "x_stacked\n",
    "print(f\"Previous tensor: {x_reshaped}\")\n",
    "print(f\"Previous shape: {x_reshaped.shape}\")\n",
    "\n",
    "# Remove extra dimension from x_reshaped\n",
    "x_squeezed = x_reshaped.squeeze()\n",
    "print(f\"\\nNew tensor: {x_squeezed}\")\n",
    "print(f\"New shape: {x_squeezed.shape}\")\n",
    "\n",
    "print(f\"Previous tensor: {x_squeezed}\")\n",
    "print(f\"Previous shape: {x_squeezed.shape}\")\n",
    "\n",
    "## Add an extra dimension with unsqueeze\n",
    "x_unsqueezed = x_squeezed.unsqueeze(dim=0)\n",
    "print(f\"\\nNew tensor: {x_unsqueezed}\")\n",
    "print(f\"New shape: {x_unsqueezed.shape}\")\n",
    "\n",
    "# Create tensor with specific shape\n",
    "x_original = torch.rand(size=(224, 224, 3))\n",
    "\n",
    "# Permute the original tensor to rearrange the axis order\n",
    "x_permuted = x_original.permute(2, 0, 1) # shifts axis 0->1, 1->2, 2->0\n",
    "\n",
    "print(f\"Previous shape: {x_original.shape}\")\n",
    "print(f\"New shape: {x_permuted.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing (selecting data from tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tensor \n",
    "import torch\n",
    "x = torch.arange(1, 10).reshape(1, 3, 3)\n",
    "x, x.shape\n",
    "\n",
    "# Let's index bracket by bracket\n",
    "print(f\"First square bracket:\\n{x[0]}\") \n",
    "print(f\"Second square bracket: {x[0][0]}\") \n",
    "print(f\"Third square bracket: {x[0][0][0]}\")\n",
    "\n",
    "# Get all values of 0th dimension and the 0 index of 1st dimension\n",
    "x[:, 0]\n",
    "\n",
    "# Get all values of 0th & 1st dimensions but only index 1 of 2nd dimension\n",
    "x[:, :, 1]\n",
    "\n",
    "# Get all values of the 0 dimension but only the 1 index value of the 1st and 2nd dimension\n",
    "x[:, 1, 1]\n",
    "\n",
    "# Get index 0 of 0th and 1st dimension and all values of 2nd dimension \n",
    "x[0, 0, :] # same as x[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  PyTorch tensors & NumPy\n",
    "The two main methods you'll want to use for NumPy to PyTorch (and back again) are:\n",
    "\n",
    "torch.from_numpy(ndarray) - NumPy array -> PyTorch tensor.\n",
    "torch.Tensor.numpy() - PyTorch tensor -> NumPy array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.Tensor.numpy() - PyTorch tensor -> NumPy array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: By default, NumPy arrays are created with the datatype float64 and if you convert it to a PyTorch tensor, it'll keep the same datatype (as above).\n",
    "\n",
    "However, many PyTorch calculations default to using float32.\n",
    "\n",
    "So if you want to convert your NumPy array (float64) -> PyTorch tensor (float64) -> PyTorch tensor (float32), you can use \n",
    "             tensor = torch.from_numpy(array).type(torch.float32)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy array to tensor\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "array = np.arange(1.0, 8.0)\n",
    "tensor = torch.from_numpy(array)\n",
    "array, tensor\n",
    "\n",
    "# Change the array, keep the tensor\n",
    "array = array + 1\n",
    "array, tensor\n",
    "\n",
    "# Tensor to NumPy array\n",
    "tensor = torch.ones(7) # create a tensor of ones with dtype=float32\n",
    "numpy_tensor = tensor.numpy() # will be dtype=float32 unless changed\n",
    "tensor, numpy_tensor\n",
    "\n",
    "# Change the tensor, keep the array the same\n",
    "tensor = tensor + 1\n",
    "tensor, numpy_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducibility (trying to take the random out of random)\n",
    "How does this relate to neural networks and deep learning then?\n",
    "\n",
    "We've discussed neural networks start with random numbers to describe patterns in data (these numbers are poor descriptions) and try to improve those random numbers using tensor operations (and a few other things we haven't discussed yet) to better describe patterns in data.\n",
    "\n",
    "In short:\n",
    "\n",
    "start with random numbers -> tensor operations -> try to make better (again and again and again)\n",
    "\n",
    "Although randomness is nice and powerful, sometimes you'd like there to be a little less randomness.\n",
    "\n",
    "Why?\n",
    "\n",
    "So you can perform repeatable experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Create two random tensors\n",
    "random_tensor_A = torch.rand(3, 4)\n",
    "random_tensor_B = torch.rand(3, 4)\n",
    "\n",
    "print(f\"Tensor A:\\n{random_tensor_A}\\n\")\n",
    "print(f\"Tensor B:\\n{random_tensor_B}\\n\")\n",
    "print(f\"Does Tensor A equal Tensor B? (anywhere)\")\n",
    "random_tensor_A == random_tensor_B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what if you wanted to created two random tensors with the same values.\n",
    "\n",
    "As in, the tensors would still contain random values but they would be of the same flavour.\n",
    "\n",
    "That's where torch.manual_seed(seed) comes in, where seed is an integer (like 42 but it could be anything) that flavours the randomness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "# # Set the random seed\n",
    "RANDOM_SEED=42 # try changing this to different values and see what happens to the numbers below\n",
    "torch.manual_seed(seed=RANDOM_SEED) \n",
    "random_tensor_C = torch.rand(3, 4)\n",
    "\n",
    "# Have to reset the seed every time a new rand() is called \n",
    "# Without this, tensor_D would be different to tensor_C \n",
    "torch.random.manual_seed(seed=RANDOM_SEED) # try commenting this line out and seeing what happens\n",
    "random_tensor_D = torch.rand(3, 4)\n",
    "\n",
    "print(f\"Tensor C:\\n{random_tensor_C}\\n\")\n",
    "print(f\"Tensor D:\\n{random_tensor_D}\\n\")\n",
    "print(f\"Does Tensor C equal Tensor D? (anywhere)\")\n",
    "random_tensor_C == random_tensor_D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resource: What we've just covered only scratches the surface of reproducibility in PyTorch. For more, on reproducbility in general and random seeds, I'd checkout:\n",
    "\n",
    "    The PyTorch reproducibility documentation: (a good exericse would be to read through this for 10-minutes and even if you don't understand it now, being aware of it is important).\n",
    "\n",
    "    The Wikipedia random seed page: (this'll give a good overview of random seeds and pseudorandomness in general)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running tensors on GPUs (and making faster computations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue May 21 10:02:02 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.43.04    Driver Version: 515.43.04    CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:1B:00.0 Off |                  N/A |\n",
      "| 72%   77C    P2   257W / 350W |   6021MiB / 24576MiB |     51%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  Off  | 00000000:B2:00.0 Off |                  N/A |\n",
      "| 74%   77C    P2   341W / 350W |  21499MiB / 24576MiB |    100%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA GeForce ...  Off  | 00000000:DB:00.0 Off |                  N/A |\n",
      "| 53%   67C    P2   263W / 350W |   5101MiB / 24576MiB |     52%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      3648      G   /usr/libexec/Xorg                  27MiB |\n",
      "|    0   N/A  N/A      4486      G   /usr/bin/gnome-shell               12MiB |\n",
      "|    0   N/A  N/A     42088      C   python                            881MiB |\n",
      "|    0   N/A  N/A    623852      C   python                           5095MiB |\n",
      "|    1   N/A  N/A      3648      G   /usr/libexec/Xorg                   4MiB |\n",
      "|    1   N/A  N/A   3924471      C   ...onda3/envs/dsd/bin/python    21491MiB |\n",
      "|    2   N/A  N/A      3648      G   /usr/libexec/Xorg                   4MiB |\n",
      "|    2   N/A  N/A    622494      C   python                           5093MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for GPU\n",
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Set device type\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count number of devices\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Device-agnostic code \n",
    "##pytorch official website\n",
    "Due to the structure of PyTorch, you may need to explicitly write device-agnostic (CPU or GPU) code; an example may be creating a new tensor as the initial hidden state of a recurrent neural network.\n",
    "\n",
    "The first step is to determine whether the GPU should be used or not. A common pattern is to use Pythonâ€™s argparse module to read in user arguments, and have a flag that can be used to disable CUDA, in combination with is_available(). In the following, args.device results in a torch.device object that can be used to move tensors to CPU or CUDA.\n",
    "\n",
    " CODE:-\n",
    "        import argparse\n",
    "        import torch\n",
    "\n",
    "        parser = argparse.ArgumentParser(description='PyTorch Example')\n",
    "        parser.add_argument('--disable-cuda', action='store_true',\n",
    "                            help='Disable CUDA')\n",
    "        args = parser.parse_args()\n",
    "        args.device = None\n",
    "        if not args.disable_cuda and torch.cuda.is_available():\n",
    "            args.device = torch.device('cuda')\n",
    "        else:\n",
    "            args.device = torch.device('cpu')\n",
    "\n",
    "USAGE:-\n",
    "\n",
    "Now that we have args.device, we can use it to create a Tensor on the desired device.\n",
    "\n",
    "        x = torch.empty((8, 42), device=args.device)\n",
    "        net = Network().to(device=args.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting tensors (and models) on the GPU\n",
    "\n",
    "Note: Putting a tensor on GPU using to(device) (e.g. some_tensor.to(device)) returns a copy of that tensor, e.g. the same tensor will be on CPU and GPU. To overwrite tensors, reassign them:\n",
    "\n",
    "some_tensor = some_tensor.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tensor (default on CPU)\n",
    "tensor = torch.tensor([1, 2, 3])\n",
    "\n",
    "# Tensor not on GPU\n",
    "print(tensor, tensor.device)\n",
    "\n",
    "# Move tensor to GPU (if available)\n",
    "tensor_on_gpu = tensor.to(device)\n",
    "tensor_on_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moving tensors back to the CPU\n",
    "\n",
    "If tensor is on GPU, can't transform it to NumPy (this will error)\n",
    "    tensor_on_gpu.numpy()\n",
    "\n",
    "TypeError: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead, copy the tensor back to cpu\n",
    "tensor_back_on_cpu = tensor_on_gpu.cpu().numpy()\n",
    "tensor_back_on_cpu\n",
    "\n",
    "#The above returns a copy of the GPU tensor in CPU memory so the original tensor is still on GPU.\n",
    "tensor_on_gpu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
